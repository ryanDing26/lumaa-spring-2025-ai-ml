{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3803f6-ced2-4511-87a0-9595a0041abf",
   "metadata": {},
   "source": [
    "# 1. Install necessary requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487b14a-96d9-4f3f-838a-0b060a8e32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb0330-9be5-4463-8486-78ac227f49ba",
   "metadata": {},
   "source": [
    "# 2. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa0a403-4d3e-4e2d-8909-fed1a0bad214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23280c15-13d7-40d6-bef9-5ad5fcc3e1ab",
   "metadata": {},
   "source": [
    "# 3. Implement Content-Based Recommendation Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0dac75f-20ef-4f34-9355-9126dcf0c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Preprocesses a given csv file for the task.\n",
    "\n",
    "    :param dataset: Path to csv file to preprocess.\n",
    "    :type dataset: str\n",
    "    :returns: Preprocessed DataFrame object.\n",
    "    :rtype: pd.DataFrame\n",
    "    '''\n",
    "    # Read in file, drop unnecessary and missing rows and columns, and sample 500 examples at random\n",
    "    df = pd.read_csv('movies.csv')[['Title', 'Plot']].dropna()\n",
    "    df = df.sample(n=500, random_state=22)\n",
    "    return df\n",
    "\n",
    "def vectorize(text: list[str], vectorizer: TfidfVectorizer):\n",
    "    '''\n",
    "    Vectorizes a given input text given a fitted vectorizer.\n",
    "\n",
    "    :param text: Text to vectorize.\n",
    "    :type text: list[str]\n",
    "    :param vectorizer: Fitted vectorizer to transform text.\n",
    "    :type vectorizer: TfidfVectorizer()\n",
    "    :returns: Vectorized text.\n",
    "    :rtype: scipy.sparse._csr.csr_matrix\n",
    "    '''\n",
    "    return vectorizer.transform(text)\n",
    "    \n",
    "def compute_similarity(a: str, b: str | list) -> np.ndarray:\n",
    "    '''\n",
    "    Computes the cosine similarity between two items.\n",
    "    \n",
    "    :param a: First text to compute similarity with.\n",
    "    :type a: str\n",
    "    :param b: Second text to compute similarity with.\n",
    "    :type b: str | list\n",
    "    :returns: Similarity score between 0 and 1 of the two items\n",
    "    :rtype: 1D np.ndarray (float)\n",
    "    '''\n",
    "    return cosine_similarity(a, b).flatten()\n",
    "    \n",
    "def recommend_movies(df: pd.DataFrame, review: str, vectorizer: TfidfVectorizer) -> list:\n",
    "    '''\n",
    "    Given a user review, recommend movies based on the similarity metric.\n",
    "\n",
    "    :param df: DataFrame containing movies and corresponding summaries.\n",
    "    :type df: pd.DataFrame\n",
    "    :param review: User sentiment on movies.\n",
    "    :type review: str\n",
    "    :param vectorizer: Fitted vectorizer object to transformed text.\n",
    "    :type vectorizer: TfIdfVectorizer()\n",
    "    :returns: Movies sorted in descending similarity score.\n",
    "    :rtype: list of (title, similarity score) tuples.\n",
    "    '''\n",
    "    # Obtain vectorized dataset and user review\n",
    "    vectorized_plots = vectorize(df['Plot'], vectorizer)\n",
    "    vectorized_review = vectorize([review], vectorizer)\n",
    "\n",
    "    # Calculate and sort titles from high to low similarity scores\n",
    "    similarity_scores = compute_similarity(vectorized_review, vectorized_plots)\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "\n",
    "    # Format and return recommendations\n",
    "    recommendations = [(df['Title'].iloc[idx], similarity_scores[idx]) for idx in sorted_indices]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea6e96-d7a7-4bfd-8d0c-66433a534a55",
   "metadata": {},
   "source": [
    "# 4. Recommend Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ca7789-3d64-4d4b-a26c-a19d33c873a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset = './movies.csv'\n",
    "    # Read in a user review and preprocess the dataset\n",
    "    user_review = input('Enter user review here:')\n",
    "    df = preprocess_data(dataset)\n",
    "\n",
    "    # Init and fit vectorizer with the custom tokenization scheme\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(df['Plot'])\n",
    "    \n",
    "    # Recommend users movies\n",
    "    user_recommendations = recommend_movies(df, user_review, vectorizer)\n",
    "    \n",
    "    print('Top 5 Movie Recommendations')\n",
    "    for i in range(5): print(f'{i+1}. {user_recommendations[i][0]} | Similarity Score: {user_recommendations[i][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "744add82-d3fa-4966-b699-0a1b56d306d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter user review here: I like comedy films\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Movie Recommendations\n",
      "1. Best Actor | Similarity Score: 0.08575388093328194\n",
      "2. Alice in Wonderland | Similarity Score: 0.06601668577160774\n",
      "3. The Band Wagon | Similarity Score: 0.0447180503457958\n",
      "4. Major Barbara | Similarity Score: 0.03407259752676045\n",
      "5. An Angel from Texas | Similarity Score: 0.03352653805024108\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648059b-5fa8-4d11-af78-20d48d0a7ce6",
   "metadata": {},
   "source": [
    "## Additional Design Considerations / Experimentation Details\n",
    "\n",
    "1. **Using stemmers, removing punctuation, and general text preprocessing**\n",
    "\n",
    "There are different improvements that can be made with bag-of-words based vectorization schemes such as that implemented in TfidfVectorizer. In performing this task, I experimented with stemming, removing stopwords, and other textual preprocessing. However, many of these coincided with a decrease in performance in the recommendation system due to conflicts with TfidfVectorizer's internal tokenization and fitting scheme (non-empirically measured via sanity checks on the top 5 recommendations), leading to suboptimal recommendations. Thus, I discarded these changes and opted for a barebone approach with just the vectorizer. \n",
    "\n",
    "Keeping text in its barebone format may lead to ease of experimentation with other models that are attention-based, where the data needs to be kept as-is to take into account the semantic meaning and position of words in a text.\n",
    "\n",
    "2. **Vectorization scheme**\n",
    "\n",
    "This slightly ties to the previous point about preprocessing in that different vectorization schemes can lead to different results. TfidfVectorizer is by no means the only vectorization scheme suitable for this task, and other attention-based methods like [SBERT](https://sbert.net/), which take into account semantic meaning, can also be used. However, due to simplicity constraints, this model was shown, though the code is modularized for ease of replacement if needed.\n",
    "\n",
    "3. **Modularizing portions of code**\n",
    "\n",
    "While many methods may seem useless to modularize, there were design choices in play for each function:\n",
    "\n",
    "- `preprocess_data()`: Performed all the necessary dataset reduction to fit specifications of problem; easily interchangeable for other datasets\n",
    "- `vectorize()`: Performs the vectorization procedure; modularized according to the example given in the README.\n",
    "- `compute_similarity()`: Computes the similarity score according to a specific metric; interchangeable for different similarity metrics.\n",
    "- `recommend_movies()`: Handles and formats the recommendation aspect of the code nicely such that not everything is located in `main()`\n",
    "\n",
    "There were some parts which depend on what libraries one uses to accomplish this task as well as things that did not fit nicely into one of the general functions above (namely the lines pertaining to fitting the vectorizer). For that reason, I placed it in `main()` so as to not overly complicate this coding challenge too heavily.\n",
    "\n",
    "4. **Docstrings, Parameter Types, Return Types**\n",
    "\n",
    "I've worked on alot of projects where the codebase I inherited was large and the documentation small, and it takes too much time and effort to just learn how everything works. I am someone who cares about the longevity and understanding of the code and the product to both developers and consumers, and I would much rather overspecify than underspecify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245b368-e0b1-4879-a765-49225fb69c92",
   "metadata": {},
   "source": [
    "## Salary Expectation\n",
    "\n",
    "I would be fine with 1600 per month, or the base 20 per hour. I would just be excited at the opportunity to work for a Neo Accelerator-backed company!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
